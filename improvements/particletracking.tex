%GENERAL LAYOUT
%
%Why we want tracking 
%
%The differences between tracking and tracking
%
%The methods used
%
%The limitations
%
%1) Why we want tracking

One of the most time consuming aspects as well as mentally draining is manually tracking a particle. Depending on the flow rate and the number of stretches and runs desired for a particle it can take several hours. Thus one of the primary targets for improvement as discussed by Johansson \cite{AntonThesis} was to try and make the camera tracking automatic. This would enable faster measurements as well as more measurements since it would reduce fatigue. 

Such a tracking was implemented using Python and the external packages \texttt{OPENCV}, \texttt{NumPy}, \texttt{SciPy}, \texttt{ImageMagick} and \texttt{ctypes}. The goal of the tracking is relatively similar to the tracking described in \ref{sec:particleTracking} and more in detail in Johansson \cite{AntonThesis} however there are a few very important differences that produce unique problems. 

\subsection{Aquiring the image}
The first step in this is to acquire the image from the micrscope in order to identify (and track) the particle. However the Leica DFC350 FX camera only works with the proprietary Leica software which means there is no easy way to get this image straight from the camera in real time. This meant we were forced to use the \texttt{ImageGrabber} package in \texttt{Python} and then isolate the camera image from the screen. This quite easy to do but takes ca 50ms per frame which would be unnecessary for an open source camera software. 

\subsection{Removing noise}
The first step is to reduce the static noise from the movie caused by dirt, scratches and other defects in the microscope and on the camera lens as can be seen in figure \ref{fig:origFrame}. As the noise is static and everything else changes this is a simple matter of computing an average frame

\begin{equation}\label{eq:averageFrame}
\bar{F} = \frac{\sum\limits_{n=1}^{N} F }{N}
\end{equation}

an example of such an average frame can be seen in figure \ref{fig:averageFrame}. This is then removed from the camera frame and the result can be seen in figure \ref{fig:fixedFrame}. After this we apply a smoothing function and Canny edge detection \cite{Canny} and then use the resulting edge. 

\subsection{Contour detection and selection}

Once an edge image has been generated, we use the OpenCV command, \texttt{Contours} which returns a list of every contiguous group of edge pixels. If we have chosen the threshold values to the edge detection correctly, this should include the particle or a good approximation of it. 

In order to find the correct contour  a few techniques are used.

First contours whose total size is less than some minimum value, $ n_{min}$ or larger than some maximum value $n_{max}$ are ignored. Then the position $P_i$ of each contour $C_i={p_1,p_2...p_n}$ is calculated as the average pixel position

\[
P_i = \sum_{j}^n p_j/n
\]
This position is compared to the expected position of the particle , which the very first frame is the middle position and thereafter is assumed to have constant speed. 

Finally a 'thinness value' is calculated according to eq \ref{eq:thinness}

\begin{equation}\label{eq:thinness}
w_{thin}\left(\frac{ n}{d_{max}^2}\right)^2
\end{equation}. 
where $w_{thin}$ is a weighting constant, $n$ is the number of pixels in the contour and $d_{max}$ is the longest distance between two pixels in the contour.
% Mostwhere I am not really sure I should do this now that I have so few particles, but I do it none the less!

\subsection{Adjusting the Camera velocity}
Once detected twice the particle will have some velocity relative to the camera $v_{rel}$ and a position $\mathbb{P}$. 
If the velocity is larger than some threshold $v_{thresh}$ or the position is outside a center square in the image, $\mathbb{P} \not \in \mathbf{B}$ we want to adjust the velocity of the step engine. 

We then simply do a straight correction but with a damping factor $\zeta$ to prevent a feedback loop. So our resulting velocity change $V_c$ is 

\begin{equation}
V_c = v_{rel}\cdot \zeta
\end{equation}

\subsection{Time Considerations}\label{sec:time considerations}
A higher frame rate will allow for greater predictive power and increase stability as the error between frames is reduces. So reducing computational time of each task is important for optimizing the tracking which also means knowing what tasks are the most demanding. A list of the different tasks and their average execution times can be seen in table \ref{tab:benchmarks}

NOTE CURRENTLY NOT PUT IN ACTUAL DATA ONLY APPROXIMATE
\begin{table}[H]
 \begin{tabular}{l | c | c } 
 Task  			&  Average time & Std deviation \\
 Capture screen & 1000 			& 200 \\
 Find edges 	& 200			& 20 \\
 Change velocity& 400			& 50 \\
 \end{tabular}
 \caption{}
 \label{tab:benchmarks}
\end{table}

We see that the FPS is limited primarily by three routines: The screen capture routine, the change velocity routine and finally the save position routine. The first and last are unavoidable and must be done every frame by definition if we are interested in knowing the particles position as well as possible. This means we simply want to use the velocity correction as little as possible. Since the time constraint is in the communication with the step engine, there is not any optimization to be done here, at least not within the scope is this thesis. 
